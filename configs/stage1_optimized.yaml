# Stage 1 Optimization: Foundation Upgrades
# Target: Spearman 0.50-0.52 (+18-22% improvement)
#
# Key Changes from Baseline:
# 1. ESM2-650M backbone (65x larger)
# 2. Ranking loss for direct Spearman optimization
# 3. Multi-epoch training with cosine schedule
# 4. Gradient accumulation for effective batch size
# 5. Optimized regularization

# Model Configuration
model:
  esm2_model: 'facebook/esm2_t33_650M_UR50D'  # Upgrade from 8M to 650M
  num_experts: 8                               # Keep MoE architecture
  expert_hidden_dim: 512                       # Increase capacity
  dropout: 0.15                                # Slightly higher for large model
  freeze_backbone: false                       # Fine-tune the backbone
  use_gradient_checkpointing: true             # Save memory

# Data Configuration
data:
  train_dir: 'data/raw/Train_split'
  test_dir: 'data/raw/Test_split'
  max_sequence_length: 4096
  min_samples: 10
  support_frac: 0.85                           # More support data
  num_workers: 4                               # More workers for large model

# Training Configuration
training:
  # Multi-epoch training
  task_epochs: 3                               # 3 epochs per task (vs 1)

  # Batch size and accumulation
  batch_size: 1                                # Small batch for 650M model
  gradient_accumulation_steps: 4               # Effective batch size = 4

  # Optimization
  optimizer: 'adamw'                           # AdamW optimizer
  task_learning_rate: 0.000005                 # Lower LR for large model (5e-6)
  weight_decay: 0.02                           # Stronger regularization
  grad_clip_norm: 1.0

  # Learning rate schedule
  lr_scheduler: 'cosine'                       # Cosine annealing
  warmup_steps: 100                            # Warmup for stability

  # Loss function
  loss_type: 'combined'                        # MSE + Ranking
  mse_weight: 0.5                              # Weight for MSE loss
  ranking_weight: 0.5                          # Weight for ranking loss

  # Advanced training
  mixed_precision: true                        # Use AMP
  use_ema: true                                # Exponential moving average
  ema_decay: 0.999                             # EMA decay rate

  # Reproducibility
  seed: 42

# Logging Configuration
logging:
  log_interval: 1
  save_interval: 5                             # Save more frequently
  eval_interval: 10                            # Evaluate every 10 tasks
  wandb_project: 'protein-fitness-stage1'      # W&B logging (optional)

# Optimization Settings
optimization:
  early_stopping: true
  patience: 50                                 # Stop if no improvement
  metric: 'spearman'                           # Optimize for Spearman
