# Configuration for Episodic Meta-Learning with ESM2 + MoE
# Optimized version of final_script.py

# Model Configuration
model:
  esm2_model: 'facebook/esm2_t6_8M_UR50D'  # ESM2 model variant
  # Options: esm2_t6_8M_UR50D, esm2_t12_35M_UR50D, esm2_t30_150M_UR50D, esm2_t33_650M_UR50D
  num_experts: 8                            # Number of expert networks
  hidden_dim: 256                           # Hidden dimension for experts
  dropout: 0.1                              # Dropout rate
  freeze_backbone: false                    # Whether to freeze ESM2 weights

# Data Configuration
data:
  train_dir: 'data/raw/Train_split'         # Training CSV directory
  test_dir: 'data/raw/Test_split'           # Test CSV directory
  max_sequence_length: 4096                 # Maximum sequence length
  min_samples: 10                           # Minimum samples per task
  support_frac: 0.8                         # Fraction of data for support set
  num_workers: 2                            # DataLoader workers

# Training Configuration
training:
  task_epochs: 1                            # Epochs per task during meta-training
  batch_size: 4                             # Batch size
  task_learning_rate: 0.00001               # Learning rate for task adaptation
  weight_decay: 0.01                        # Weight decay
  grad_clip_norm: 1.0                       # Gradient clipping norm
  mixed_precision: true                     # Use automatic mixed precision
  seed: 42                                  # Random seed

# Logging Configuration
logging:
  log_interval: 1                           # Log every N epochs during task training
  save_interval: 10                         # Save checkpoint every N tasks
