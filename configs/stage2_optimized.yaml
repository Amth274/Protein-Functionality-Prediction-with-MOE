# Stage 2 Optimization: MSA Integration
# Target: Spearman 0.60-0.63 (+18-22% over Stage 1)
#
# Key Changes from Stage 1:
# 1. MSA feature integration (PSSM, conservation)
# 2. Multi-modal fusion architecture
# 3. Cross-attention between sequence and MSA
# 4. Continued use of ranking loss and large backbone

# Model Configuration
model:
  esm2_model: 'facebook/esm2_t33_650M_UR50D'  # Keep large backbone
  num_experts: 8
  expert_hidden_dim: 512
  msa_dim: 20                                  # PSSM dimension
  dropout: 0.15
  freeze_esm2: false
  use_gradient_checkpointing: true
  use_msa: true                                # Enable MSA features

# MSA Configuration
msa:
  enabled: true
  method: 'simple'                             # 'simple', 'blast', or 'mmseqs2'
  max_sequences: 256                           # Number of homologs
  evalue_threshold: 0.00001                    # E-value cutoff
  mmseqs_db_path: null                         # Path to MMseqs2 database (if using)
  feature_type: 'pssm'                         # 'pssm', 'conservation', or 'all'

# Data Configuration
data:
  train_dir: 'data/raw/Train_split'
  test_dir: 'data/raw/Test_split'
  max_sequence_length: 4096
  min_samples: 10
  support_frac: 0.85
  num_workers: 4

# Training Configuration
training:
  # Multi-epoch training
  task_epochs: 3

  # Batch size and accumulation
  batch_size: 1
  gradient_accumulation_steps: 4

  # Optimization
  optimizer: 'adamw'
  task_learning_rate: 0.000003                 # Even lower LR for multi-modal
  weight_decay: 0.02
  grad_clip_norm: 1.0

  # Learning rate schedule
  lr_scheduler: 'cosine'
  warmup_steps: 150                            # More warmup for stability

  # Loss function (combined MSE + Ranking)
  loss_type: 'combined'
  mse_weight: 0.4                              # Less weight on MSE
  ranking_weight: 0.6                          # More weight on ranking

  # Advanced training
  mixed_precision: true
  use_ema: true
  ema_decay: 0.999

  # Reproducibility
  seed: 42

# Logging Configuration
logging:
  log_interval: 1
  save_interval: 5
  eval_interval: 10
  wandb_project: 'protein-fitness-stage2'

# Optimization Settings
optimization:
  early_stopping: true
  patience: 50
  metric: 'spearman'
