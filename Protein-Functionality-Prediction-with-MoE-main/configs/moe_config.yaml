# Configuration for Protein Functionality MoE Training
# This file contains all hyperparameters and settings for training

# Model Architecture Configuration
model:
  input_dim: 320                    # ESM2 embedding dimension
  hidden_dim: 512                   # Hidden dimension for MoE layers
  num_experts: 4                    # Number of expert networks
  top_k: 2                         # Number of experts to use per input
  num_layers: 2                    # Number of MoE layers
  output_dim: 1                    # Output dimension (1 for regression)
  dropout: 0.1                     # Dropout probability
  expert_type: 'protein'           # Type of experts ('standard' or 'protein')
  gate_type: 'linear'              # Gating mechanism ('linear' or 'attention')
  load_balancing: true             # Whether to use load balancing loss

# Data Configuration
data:
  data_dir: 'data/raw'             # Directory containing raw CSV files
  embedding_dir: 'data/embeddings' # Directory containing pre-computed embeddings
  batch_size: 32                   # Training batch size
  num_workers: 4                   # Number of data loading workers
  support_size: 128                # Size of support set for meta-learning
  query_size: 72                   # Size of query set for meta-learning
  min_samples: 200                 # Minimum samples required per task
  max_train_files: null            # Maximum training files to use (null = all)
  max_val_files: 50                # Maximum validation files to use
  pin_memory: true                 # Pin memory for data loading
  persistent_workers: true         # Keep workers alive between epochs

# Training Configuration
training:
  epochs: 100                      # Number of training epochs
  learning_rate: 1e-4              # Initial learning rate
  weight_decay: 0.01               # Weight decay for regularization
  min_lr: 1e-6                     # Minimum learning rate for scheduler
  grad_clip_norm: 1.0              # Gradient clipping norm
  seed: 42                         # Random seed for reproducibility
  save_interval: 10                # Save checkpoint every N epochs

  # Early stopping configuration
  early_stopping:
    patience: 15                   # Epochs to wait for improvement
    min_delta: 1e-4                # Minimum change to consider improvement
    mode: 'min'                    # 'min' for loss, 'max' for metrics
    restore_best_weights: true     # Restore best weights when stopping

  # Loss function configuration
  loss:
    margin: 0.1                    # Margin for ranking loss
    n_pairs: 1000                  # Number of pairs to sample for ranking loss
    ranking_weight: 1.0            # Weight for ranking loss
    use_mse: false                 # Whether to add MSE loss
    mse_weight: 0.1                # Weight for MSE loss (if used)
    aux_weight: 0.01               # Weight for auxiliary MoE loss

  # Optimizer configuration
  optimizer:
    type: 'AdamW'                  # Optimizer type
    betas: [0.9, 0.999]           # Adam beta parameters
    eps: 1e-8                      # Adam epsilon
    amsgrad: false                 # Whether to use AMSGrad variant

  # Learning rate scheduler configuration
  scheduler:
    type: 'cosine'                 # Scheduler type ('cosine', 'step', 'warmup')
    warmup_steps: 1000             # Warmup steps (if using warmup scheduler)
    step_size: 30                  # Step size for step scheduler
    gamma: 0.1                     # Multiplicative factor for step scheduler

# Logging Configuration
logging:
  log_interval: 100                # Log training metrics every N steps
  eval_interval: 1                 # Evaluate on validation set every N epochs
  save_interval: 10                # Save checkpoint every N epochs
  tensorboard: true                # Enable tensorboard logging
  wandb: false                     # Enable Weights & Biases logging
  wandb_project: 'protein-moe'     # W&B project name
  log_level: 'INFO'                # Logging level

# Paths Configuration
paths:
  output_dir: 'outputs'            # Base output directory
  checkpoint_dir: 'outputs/checkpoints' # Checkpoint directory
  log_dir: 'outputs/logs'          # Log directory
  tensorboard_dir: 'outputs/tensorboard' # Tensorboard log directory

# Evaluation Configuration
evaluation:
  metrics:
    - 'spearman'                   # Spearman correlation
    - 'pearson'                    # Pearson correlation
    - 'mse'                        # Mean squared error
    - 'mae'                        # Mean absolute error
    - 'ndcg_at_10'                 # NDCG@10
    - 'top_k_accuracy_10'          # Top-k accuracy

  # Test set evaluation
  test_interval: 10                # Evaluate on test set every N epochs
  save_predictions: true           # Save predictions for analysis

# Hardware Configuration
hardware:
  mixed_precision: true            # Use automatic mixed precision
  distributed: false               # Enable distributed training
  find_unused_parameters: false    # For DistributedDataParallel

  # Memory optimization
  gradient_checkpointing: false    # Enable gradient checkpointing
  pin_memory: true                 # Pin memory for data loading
  non_blocking: true               # Non-blocking data transfer

# Reproducibility Configuration
reproducibility:
  deterministic: true              # Use deterministic algorithms
  benchmark: false                 # Disable cudnn benchmark for reproducibility

# Meta-learning specific configuration
meta_learning:
  enable: true                     # Enable meta-learning approach
  inner_lr: 1e-3                   # Learning rate for inner loop
  inner_steps: 5                   # Number of inner loop steps
  adaptation_strategy: 'context'   # How to adapt to new tasks ('context', 'gradient')
  context_conditioning: true       # Use support set for context conditioning

# Data augmentation (if applicable)
augmentation:
  enable: false                    # Enable data augmentation
  noise_std: 0.01                  # Standard deviation for noise augmentation
  dropout_prob: 0.1                # Probability for random dropout augmentation

# Hyperparameter search configuration (for future use)
hyperparameter_search:
  enable: false                    # Enable hyperparameter search
  method: 'optuna'                 # Search method ('optuna', 'ray')
  n_trials: 100                    # Number of trials

  # Search spaces
  search_space:
    learning_rate: [1e-5, 1e-3]
    hidden_dim: [256, 512, 1024]
    num_experts: [2, 4, 8]
    dropout: [0.05, 0.2]